{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SuperResoluteCNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "j_-xQVEZeCYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "img=image.load_img('Input_resolution.jpg')\n",
        "x=image.img_to_array(img)\n",
        "x.shape\n",
        "import numpy as np\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x.shape\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nDHQ33qrxzx2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ISfGq2vceCZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t2aCw_rNeCZr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HLVH1J0ieCZ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0432fb91-930f-4abf-bc46-9220d630c70f"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/sample_data')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /sample_data; to attempt to forcibly remount, call drive.mount(\"/sample_data\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0RRpPTGFjmur",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ccb2589d-83b1-4c4a-ec68-b8d3cf9598e6"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "from os import path\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPool2D,Flatten\n",
        "from keras.layers.core import Dense,Dropout,Activation\n",
        "from keras.optimizers import adam\n",
        "from keras.utils import np_utils\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1blPuFdfeCZ9",
        "colab_type": "code",
        "outputId": "5490496b-945c-4f4f-bdf3-159392c1d775",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Reference Book:  [Navin_Kumar_Manaswi]_Deep_Learning_with_Applicati(z-lib.org).pdf\n",
        "\n",
        "\n",
        "original_data_path='/sample_data'\n",
        "\n",
        "\n",
        "\n",
        "def load_small_img_dataset(path=os.getcwd()):\n",
        "    data=[]\n",
        "    for image in os.listdir(path):\n",
        "        if image.endswith(('.jpg','.jpeg','.png','bmp'),0,len(image)):\n",
        "            pixels=cv2.imread(os.path.join(path,image))\n",
        "            pixels=pixels[0:100,0:90]  #first make all images of same size using crop\n",
        "            data.append(pixels)\n",
        "            \n",
        "    return data\n",
        "    \n",
        "imgs=load_small_img_dataset(original_data_path)\n",
        "            \n",
        "# OR import keras dataset       from keras.datasets import cifar10  \n",
        "#(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#generate a random number between 0 and 1 and if it is less than 0.7 than the current image is in train set else fi >0.9 then test else dev set\n",
        "\n",
        "#or use sklearn.model_selection import train_test_split\n",
        "\n",
        "def test_train_dev_split(dataset,train=0.7,dev=0.2,test=0.1):\n",
        "    #make seed for exact results everything\n",
        "    #random.sort(dataset)\n",
        "    random.seed(2)\n",
        "    random.shuffle(dataset)\n",
        "    split1=int(train*len(dataset))\n",
        "    split2=int((train+dev)*len(dataset))\n",
        "    \n",
        "    train_set=dataset[:split1]\n",
        "    dev_set=dataset[split1:split2]\n",
        "    test_set=dataset[split2:]\n",
        "    \n",
        "    return train_set,dev_set,test_set\n",
        "    \n",
        "    \n",
        "def create_input_out_sets(train_output,dev_output,test_output):\n",
        "    train_input=[]\n",
        "    dev_input=[]\n",
        "    test_input=[]\n",
        "    for img in train_output:\n",
        "        low_pixels=cv2.resize(img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
        "        train_input.append(cv2.resize(low_pixels,None,fx=2,fy=2,interpolation=cv2.INTER_AREA))\n",
        "    \n",
        "    for img in dev_output:\n",
        "        low_pixels=cv2.resize(img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
        "        dev_input.append(cv2.resize(low_pixels,None,fx=2,fy=2,interpolation=cv2.INTER_AREA))\n",
        "    \n",
        "    for img in test_output:\n",
        "        low_pixels=cv2.resize(img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
        "        test_input.append(cv2.resize(low_pixels,None,fx=2,fy=2,interpolation=cv2.INTER_AREA))\n",
        "\n",
        "        \n",
        "    return np.array(train_input),np.array(train_output),np.array(dev_input),np.array(dev_output),np.array(test_input),np.array(test_output)\n",
        "\n",
        "#check wheather the imgs were correctly formed\n",
        "train,dev,test=test_train_dev_split(imgs)\n",
        "print(\"[INFO] Splitting done....\")\n",
        "\n",
        "train_in,train_out,dev_in,dev_out,test_in,test_out=create_input_out_sets(train,dev,test)\n",
        "print(\"[INFO] Input and output images created and sorted in datasets\")\n",
        "\n",
        "\n",
        "cv2.imshow(\"Original Img\",len(train_out))\n",
        "cv2.imshow(\"Low resolution\",train_in[1])\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"[INFO] Shape of image\",test_in[1].shape)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "RwRzG3zkeCaI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pre-Process Images :\n",
        "\n",
        "'''\n",
        "Both TensorFlow and Theano expects a 4 dimensional tensor as input. \n",
        "But where TensorFlow expects the 'channels' dimension as the last dimension \n",
        "(index 3, where the first is index 0) of the tensor – i.e. tensor with shape (samples, rows, cols, channels) – \n",
        "Theano will expect 'channels' at the second dimension (index 1) – \n",
        "i.e. tensor with shape (samples, channels, rows, cols). '''\n",
        "\n",
        "# Keras Format:: [samples][width][height][channels]\n",
        "# OpenCV format::  rows, columns and channels i.e. [height][width][channels]\n",
        "\n",
        "# Current format:: [samples][height][width][channels]   ->>>>>>   [samples][width][height][channels]\n",
        "\n",
        "\n",
        "'''# Reshape input data.\n",
        "train_in=train_in.reshape(train_in.shape[0],90,100,3)\n",
        "train_out=train_out.reshape(train_out.shape[0],90,100,3)\n",
        "dev_in=dev_in.reshape(dev_in.shape[0],90,100,3)\n",
        "dev_out=dev_out.reshape(dev_out.shape[0],90,100,3)\n",
        "test_in=test_in.reshape(test_in.shape[0],90,100,3)\n",
        "test_out=test_out.reshape(test_out.shape[0],90,100,3)\n",
        "'''\n",
        "\n",
        "# to convert our data type to float32 and normalize our database\n",
        "train_in=train_in.astype('float32')\n",
        "dev_in=dev_in.astype('float32')\n",
        "test_in=test_in.astype('float32')\n",
        "print(train_in.shape)\n",
        "print(test_in.shape)\n",
        "\n",
        "\n",
        "# Z-scoring or Gaussian Normalization\n",
        "train_in=train_in - np.mean(train_in) / train_in.std()\n",
        "dev_in=dev_in - np.mean(dev_in) / dev_in.std()\n",
        "test_in=test_in - np.mean(test_in) / test_in.std()\n",
        "\n",
        "#Saving into numpy arrays\n",
        "np.save('train_in.npy',train_in)\n",
        "np.save('train_out.npy',train_out)\n",
        "np.save('dev_in.npy',dev_in)\n",
        "np.save('dev_out.npy',dev_out)\n",
        "np.save('test_in.npy',test_in)\n",
        "np.save('test_out.npy',test_out)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t3S_XE5IjsoX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load data from saved numpy arrays\n",
        "\n",
        "\n",
        "train_in=np.load('train_in.npy')\n",
        "train_out=np.load('train_out.npy')\n",
        "dev_in=np.load('dev_in.npy')\n",
        "dev_out=np.load('dev_out.npy')\n",
        "test_in=np.load('test_in.npy')\n",
        "test_out=np.load('test_out.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h2s9T26UeCaU",
        "colab_type": "code",
        "outputId": "ed4c7d24-d348-4d9d-8a0c-3d2b0d040cd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Image Super-Resolution Using Deep\n",
        "Convolutional Networks\n",
        "Chao Dong[2015]\n",
        "'''\n",
        "\n",
        "# Define the keras DNN model\n",
        "model =Sequential()\n",
        "model.add(Conv2D(64,(9,9),input_shape=(100,90,3),activation='relu',padding='same'))\n",
        "#model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4)) \n",
        "model.add(Conv2D(32,(1,1),activation='relu',padding='same'))\n",
        "#model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2)) \n",
        "model.add(Conv2D(3,(5,5),activation='relu',padding='same'))\n",
        "#model.add(MaxPool2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3)) \n",
        "\n",
        "\n",
        "print(\"Models' output Shape: \",model.output_shape)\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Models' output Shape:  (None, 100, 90, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hndb806ZeCab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "\n",
        "# Save the model according to the conditions  \n",
        "checkpoint = ModelCheckpoint(\"resoluteitmodelCNN_colab1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
        "early = EarlyStopping(monitor='resoluteitweightsCNN_colab1', min_delta=0, patience=10, verbose=1, mode='auto')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yC-HLs0KeCag",
        "colab_type": "code",
        "outputId": "cdb60c1e-c8eb-4d47-827d-599c0c1d9a96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def psnr(y_true, y_pred):\n",
        "    '''assert y_true.shape == y_pred.shape, \"Cannot calculate PSNR. Input shapes not same.\" \\\n",
        "                                         \" y_true shape = %s, y_pred shape = %s\" % (str(y_true.shape),\n",
        "                                                                                   str(y_pred.shape))\n",
        "    '''\n",
        "    \"\"\"\n",
        "    PSNR is Peek Signal to Noise Ratio, which is similar to mean squared error.\n",
        "    It can be calculated as\n",
        "    PSNR = 20 * log10(MAXp) - 10 * log10(MSE)\n",
        "    When providing an unscaled input, MAXp = 255. Therefore 20 * log10(255)== 48.1308036087.\n",
        "    However, since we are scaling our input, MAXp = 1. Therefore 20 * log10(1) = 0.\n",
        "    Thus we remove that component completely and only compute the remaining MSE component.\n",
        "    \"\"\"\n",
        "    return -10. * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.)\n",
        "\n",
        "\n",
        "\n",
        "#compile the model algong with adam optimiser along with PSNR/SSIM loss metric\n",
        "model.compile(optimizer=adam(0.005),metrics=[psnr],loss='mse')\n",
        "model.fit(train_in,train_out,batch_size=15,nb_epoch=1000,validation_data=(dev_in,dev_out),callbacks = [early])\n",
        "'''#loading saved weights\n",
        "modelWts=model.load_weights('savedWeightsCNN.h5')\n",
        "'''\n",
        "\n",
        "#evaluate the model\n",
        "score=model.evaluate(test_in,test_out)\n",
        "print(\"[INFO] MSE:{0}   PSNRLoss:{1}\".format(score[0],score[1]))\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-6ca1b89b263f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#compile the model algong with adam optimiser along with PSNR/SSIM loss metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpsnr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdev_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mearly\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m '''#loading saved weights\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4VMhVCvveCau",
        "colab_type": "code",
        "outputId": "146f858d-f315-4428-ff19-1552c23ef271",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1216
        }
      },
      "cell_type": "code",
      "source": [
        "#print(\"TEST image shape: \",test_in[0].shape) \n",
        "\n",
        "# prediction\n",
        "\n",
        "#unknown test data\n",
        "#cv2.imshow(\"Original Img\",test_original_resolution[0])\n",
        "#cv2.imshow(\"Low resolution\",test_imgs[0])\n",
        "pred_image=model.predict(test_in[6:7])\n",
        "'''cv2.imshow(\"Peredicted resolution\",pred_image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()'''\n",
        "\n",
        "\n",
        "print(\"Peredicted resolution shape :\",pred_image[0])\n",
        "\n",
        "'''#save img\n",
        "plt.subplot(221)\n",
        "plt.imshow(test_in[2])\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.imshow(test_out[2])\n",
        "\n",
        "plt.subplot(223)\n",
        "'''\n",
        "plt.imshow(pred_image[0])\n",
        "\n",
        "\n",
        "\n",
        "cv2.imwrite(\"Original_Img_colab.jpg\",test_out[6])\n",
        "cv2.imwrite(\"Input_resolution6_colab.jpg\",test_in[6])\n",
        "cv2.imwrite(\"Test_Output6_colab.jpg\",pred_image[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Peredicted resolution shape : [[[ 19.668701  18.286865  21.661953]\n",
            "  [ 22.122768  19.893074  23.55714 ]\n",
            "  [ 20.43134   17.78014   21.421448]\n",
            "  ...\n",
            "  [ 66.09257   61.84668   98.48797 ]\n",
            "  [ 65.26206   63.11695   93.716225]\n",
            "  [ 56.582794  52.815628  79.01002 ]]\n",
            "\n",
            " [[ 19.641562  18.307236  21.687405]\n",
            "  [ 21.468842  19.323086  22.886955]\n",
            "  [ 18.838438  16.082405  19.911423]\n",
            "  ...\n",
            "  [ 66.10925   61.85     105.458885]\n",
            "  [ 70.292564  68.13384  104.841644]\n",
            "  [ 63.9906    59.654778  90.39204 ]]\n",
            "\n",
            " [[ 19.625193  15.45108   20.251368]\n",
            "  [ 19.162064  13.50333   18.794952]\n",
            "  [ 14.889638   8.377928  14.348352]\n",
            "  ...\n",
            "  [ 59.17842   53.53225  104.40384 ]\n",
            "  [ 66.549706  63.86891  106.64748 ]\n",
            "  [ 65.293724  60.165993  95.90595 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 28.853525  26.746037  43.36442 ]\n",
            "  [ 33.484695  29.664156  51.25241 ]\n",
            "  [ 40.897537  35.218117  63.14433 ]\n",
            "  ...\n",
            "  [ 12.310358   8.993461  10.260711]\n",
            "  [ 11.621489   9.596401  10.622714]\n",
            "  [ 10.996647  10.015328  10.837965]]\n",
            "\n",
            " [[ 19.463062  18.927864  29.472988]\n",
            "  [ 21.516546  19.831217  34.048473]\n",
            "  [ 27.54558   23.897078  43.372795]\n",
            "  ...\n",
            "  [ 13.467127  10.929609  11.906617]\n",
            "  [ 13.169186  11.962878  12.689775]\n",
            "  [ 12.236491  11.764582  12.404881]]\n",
            "\n",
            " [[ 13.933015  14.319071  21.032135]\n",
            "  [ 14.264543  13.887805  23.126743]\n",
            "  [ 18.766579  16.297436  29.658752]\n",
            "  ...\n",
            "  [ 12.859666  11.25099   11.750277]\n",
            "  [ 12.984744  12.219717  12.662078]\n",
            "  [ 12.054084  11.745324  12.165144]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAFMCAYAAACeS+oRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngCn/AlthL1K3smuAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x396 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TYIdrQbReCa6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''for i in range(0,len(test_in)):\n",
        "    cv2.imwrite(\"{0}.png\".format(i),test_in[i])'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0-Vo76dOeCbG",
        "colab_type": "code",
        "outputId": "4f143767-4071-460e-a156-693e76f15e9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#Save the model\n",
        "model.save('resoluteitmodelCNN_colab.h5')\n",
        "jsonmodel=model.to_json()\n",
        "model.save_weights('savedWeightsCNN_colab.h5')\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "#loading saved weights\n",
        "#modelWts=model.load_weights('savedWeightsCNN.h5')\n",
        "#model.get_weights()\n",
        "#model.get_config()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 100, 90, 64)       15616     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 100, 90, 64)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 100, 90, 32)       2080      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 100, 90, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 100, 90, 3)        2403      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 100, 90, 3)        0         \n",
            "=================================================================\n",
            "Total params: 20,099\n",
            "Trainable params: 20,099\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xQMLzpGieCbW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}