{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100, 90, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "img=image.load_img('Input_resolution.jpg')\n",
    "x=image.img_to_array(img)\n",
    "x.shape\n",
    "import numpy as np\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference Book:  [Navin_Kumar_Manaswi]_Deep_Learning_with_Applicati(z-lib.org).pdf\n",
    "\n",
    "from os import path\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPool2D,Flatten\n",
    "from keras.layers.core import Dense,Dropout,Activation\n",
    "from keras.optimizers import adam\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "original_data_path='D:\\dataset\\Image\\General-100'\n",
    "\n",
    "\n",
    "\n",
    "def load_small_img_dataset(path=os.getcwd()):\n",
    "    data=[]\n",
    "    for image in os.listdir(path):\n",
    "        if image.endswith(('.jpg','.jpeg','.png','bmp'),0,len(image)):\n",
    "            pixels=cv2.imread(os.path.join(path,image))\n",
    "            pixels=pixels[0:100,0:90]  #first make all images of same size using crop\n",
    "            data.append(pixels)\n",
    "            \n",
    "    return data\n",
    "    \n",
    "imgs=load_small_img_dataset(original_data_path)\n",
    "            \n",
    "# OR import keras dataset       from keras.datasets import cifar10  \n",
    "#(x_train, y_train), (x_test, y_test) = cifar100.load_data(label_mode='fine')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#generate a random number between 0 and 1 and if it is less than 0.7 than the current image is in train set else fi >0.9 then test else dev set\n",
    "\n",
    "#or use sklearn.model_selection import train_test_split\n",
    "\n",
    "def test_train_dev_split(dataset,train=0.7,dev=0.2,test=0.1):\n",
    "    #make seed for exact results everything\n",
    "    #random.sort(dataset)\n",
    "    random.seed(2)\n",
    "    random.shuffle(dataset)\n",
    "    split1=int(train*len(dataset))\n",
    "    split2=int((train+dev)*len(dataset))\n",
    "    \n",
    "    train_set=dataset[:split1]\n",
    "    dev_set=dataset[split1:split2]\n",
    "    test_set=dataset[split2:]\n",
    "    \n",
    "    return train_set,dev_set,test_set\n",
    "    \n",
    "    \n",
    "def create_input_out_sets(train_output,dev_output,test_output):\n",
    "    train_input=[]\n",
    "    dev_input=[]\n",
    "    test_input=[]\n",
    "    for img in train_output:\n",
    "        low_pixels=cv2.resize(img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
    "        train_input.append(cv2.resize(low_pixels,None,fx=2,fy=2,interpolation=cv2.INTER_AREA))\n",
    "    \n",
    "    for img in dev_output:\n",
    "        low_pixels=cv2.resize(img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
    "        dev_input.append(cv2.resize(low_pixels,None,fx=2,fy=2,interpolation=cv2.INTER_AREA))\n",
    "    \n",
    "    for img in test_output:\n",
    "        low_pixels=cv2.resize(img,None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
    "        test_input.append(cv2.resize(low_pixels,None,fx=2,fy=2,interpolation=cv2.INTER_AREA))\n",
    "\n",
    "        \n",
    "    return np.array(train_input),np.array(train_output),np.array(dev_input),np.array(dev_output),np.array(test_input),np.array(test_output)\n",
    "\n",
    "#check wheather the imgs were correctly formed\n",
    "train,dev,test=test_train_dev_split(imgs)\n",
    "print(\"[INFO] Splitting done....\")\n",
    "\n",
    "train_in,train_out,dev_in,dev_out,test_in,test_out=create_input_out_sets(train,dev,test)\n",
    "print(\"[INFO] Input and output images created and sorted in datasets\")\n",
    "\n",
    "\n",
    "cv2.imshow(\"Original Img\",train_out[1])\n",
    "cv2.imshow(\"Low resolution\",train_in[1])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"[INFO] Shape of image\",test_in[1].shape)\n",
    "\n",
    "\n",
    "\n",
    "#pre-Process Images :\n",
    "\n",
    "\n",
    "Both TensorFlow and Theano expects a 4 dimensional tensor as input. \n",
    "But where TensorFlow expects the 'channels' dimension as the last dimension \n",
    "(index 3, where the first is index 0) of the tensor – i.e. tensor with shape (samples, rows, cols, channels) – \n",
    "Theano will expect 'channels' at the second dimension (index 1) – \n",
    "i.e. tensor with shape (samples, channels, rows, cols). \n",
    "\n",
    "# Keras Format:: [samples][width][height][channels]\n",
    "# OpenCV format::  rows, columns and channels i.e. [height][width][channels]\n",
    "\n",
    "# Current format:: [samples][height][width][channels]   ->>>>>>   [samples][width][height][channels]\n",
    "\n",
    "\n",
    "# Reshape input data.\n",
    "train_in=train_in.reshape(train_in.shape[0],90,100,3)\n",
    "train_out=train_out.reshape(train_out.shape[0],90,100,3)\n",
    "dev_in=dev_in.reshape(dev_in.shape[0],90,100,3)\n",
    "dev_out=dev_out.reshape(dev_out.shape[0],90,100,3)\n",
    "test_in=test_in.reshape(test_in.shape[0],90,100,3)\n",
    "test_out=test_out.reshape(test_out.shape[0],90,100,3)\n",
    "\n",
    "\n",
    "# to convert our data type to float32 and normalize our database\n",
    "train_in=train_in.astype('float32')\n",
    "dev_in=dev_in.astype('float32')\n",
    "test_in=test_in.astype('float32')\n",
    "print(train_in.shape)\n",
    "print(test_in.shape)\n",
    "\n",
    "\n",
    "# Z-scoring or Gaussian Normalization\n",
    "train_in=train_in - np.mean(train_in) / train_in.std()\n",
    "dev_in=dev_in - np.mean(dev_in) / dev_in.std()\n",
    "test_in=test_in - np.mean(test_in) / test_in.std()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#Saving into numpy arrays\n",
    "np.save('train_in.npy',train_in)\n",
    "np.save('train_out.npy',train_out)\n",
    "np.save('dev_in.npy',dev_in)\n",
    "np.save('dev_out.npy',dev_out)\n",
    "np.save('test_in.npy',test_in)\n",
    "np.save('test_out.npy',test_out)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from saved numpy arrays\n",
    "\n",
    "\n",
    "train_in=np.load('train_in.npy')\n",
    "train_out=np.load('train_out.npy')\n",
    "dev_in=np.load('dev_in.npy')\n",
    "dev_out=np.load('dev_out.npy')\n",
    "test_in=np.load('test_in.npy')\n",
    "test_out=np.load('test_out.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models' output Shape:  (None, 100, 90, 3)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Image Super-Resolution Using Deep\n",
    "Convolutional Networks\n",
    "Chao Dong[2015]\n",
    "'''\n",
    "\n",
    "# Define the keras DNN model\n",
    "model =Sequential()\n",
    "model.add(Conv2D(64,(9,9),input_shape=(100,90,3),activation='relu',padding='same'))\n",
    "#model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Conv2D(32,(1,1),activation='relu',padding='same'))\n",
    "#model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3)) \n",
    "model.add(Conv2D(3,(5,5),activation='relu',padding='same'))\n",
    "#model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.3)) \n",
    "\n",
    "\n",
    "print(\"Models' output Shape: \",model.output_shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ravi\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 69 samples, validate on 20 samples\n",
      "Epoch 1/50\n",
      "69/69 [==============================] - 11s 160ms/step - loss: 14569.5709 - psnr: -41.5923 - val_loss: 22360.5537 - val_psnr: -43.4866\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 11471.7664 - psnr: -40.5678 - val_loss: 22298.6709 - val_psnr: -43.4745\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 10s 149ms/step - loss: 10604.1741 - psnr: -40.1850 - val_loss: 13119.5229 - val_psnr: -41.1682\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 10s 152ms/step - loss: 9881.9288 - psnr: -39.8654 - val_loss: 13353.0020 - val_psnr: -41.2431\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 10s 149ms/step - loss: 9065.7134 - psnr: -39.5537 - val_loss: 6747.3574 - val_psnr: -38.2764\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 11s 160ms/step - loss: 8905.2961 - psnr: -39.3676 - val_loss: 5165.4797 - val_psnr: -37.1099\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 12s 178ms/step - loss: 9705.6431 - psnr: -39.8131 - val_loss: 10535.9746 - val_psnr: -40.2038\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 10s 149ms/step - loss: 9382.5387 - psnr: -39.6950 - val_loss: 9154.1387 - val_psnr: -39.5868\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 10s 149ms/step - loss: 8889.9750 - psnr: -39.4548 - val_loss: 9623.8735 - val_psnr: -39.8156\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 8807.7379 - psnr: -39.3979 - val_loss: 12448.9199 - val_psnr: -40.9384\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 11s 159ms/step - loss: 8772.2843 - psnr: -39.3369 - val_loss: 10574.8901 - val_psnr: -40.2323\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 11s 163ms/step - loss: 8506.8105 - psnr: -39.2434 - val_loss: 8559.1445 - val_psnr: -39.3058\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 11s 161ms/step - loss: 8399.1607 - psnr: -39.1459 - val_loss: 7746.1750 - val_psnr: -38.8692\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 8327.0141 - psnr: -39.1720 - val_loss: 7393.9932 - val_psnr: -38.6673\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 11s 153ms/step - loss: 8225.3996 - psnr: -39.1132 - val_loss: 7300.1128 - val_psnr: -38.6077\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 11s 164ms/step - loss: 8113.5113 - psnr: -39.0752 - val_loss: 8197.7456 - val_psnr: -39.1093\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 11s 155ms/step - loss: 7994.3229 - psnr: -38.9879 - val_loss: 9654.5747 - val_psnr: -39.8226\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 12s 167ms/step - loss: 7903.2430 - psnr: -38.9377 - val_loss: 7180.3428 - val_psnr: -38.5349\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 11s 152ms/step - loss: 7848.7652 - psnr: -38.8224 - val_loss: 8930.3364 - val_psnr: -39.4870\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 7890.3194 - psnr: -38.9308 - val_loss: 7633.9453 - val_psnr: -38.8080\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 10s 151ms/step - loss: 7863.0059 - psnr: -38.8635 - val_loss: 6893.5823 - val_psnr: -38.3650\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 11s 154ms/step - loss: 7768.8388 - psnr: -38.8709 - val_loss: 7595.1248 - val_psnr: -38.7874\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 11s 153ms/step - loss: 7720.3879 - psnr: -38.7272 - val_loss: 7092.0105 - val_psnr: -38.4892\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 11s 153ms/step - loss: 7755.0123 - psnr: -38.7853 - val_loss: 8957.5918 - val_psnr: -39.5068\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 10s 152ms/step - loss: 7787.5795 - psnr: -38.8339 - val_loss: 7289.5369 - val_psnr: -38.6098\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 11s 153ms/step - loss: 7857.4506 - psnr: -38.8927 - val_loss: 8184.8621 - val_psnr: -39.1137\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 7759.9954 - psnr: -38.8716 - val_loss: 7737.2517 - val_psnr: -38.8746\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 10s 152ms/step - loss: 7703.8211 - psnr: -38.7888 - val_loss: 7400.8279 - val_psnr: -38.6834\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 7783.7427 - psnr: -38.8219 - val_loss: 8340.7310 - val_psnr: -39.1984\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 7742.0146 - psnr: -38.8090 - val_loss: 8074.6230 - val_psnr: -39.0560\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 7685.3220 - psnr: -38.8189 - val_loss: 7443.3645 - val_psnr: -38.6992\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 7698.8560 - psnr: -38.8103 - val_loss: 7518.6748 - val_psnr: -38.7471\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 10s 151ms/step - loss: 7757.0665 - psnr: -38.8612 - val_loss: 8099.5737 - val_psnr: -39.0741\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 7728.7049 - psnr: -38.8301 - val_loss: 7436.7483 - val_psnr: -38.6963\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 10s 151ms/step - loss: 7692.2816 - psnr: -38.7941 - val_loss: 7596.2617 - val_psnr: -38.7859\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 10s 152ms/step - loss: 7701.9066 - psnr: -38.7834 - val_loss: 8003.2646 - val_psnr: -39.0177\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 10s 151ms/step - loss: 7671.3892 - psnr: -38.6844 - val_loss: 7647.2683 - val_psnr: -38.8181\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 11s 153ms/step - loss: 7679.3491 - psnr: -38.7594 - val_loss: 6125.8137 - val_psnr: -37.8446\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 10s 150ms/step - loss: 7784.8164 - psnr: -38.8451 - val_loss: 9329.4580 - val_psnr: -39.6817\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 10s 151ms/step - loss: 7697.8932 - psnr: -38.8426 - val_loss: 6857.7029 - val_psnr: -38.3424\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 11s 156ms/step - loss: 7672.9731 - psnr: -38.8298 - val_loss: 7412.4028 - val_psnr: -38.6793\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 11s 153ms/step - loss: 7602.7186 - psnr: -38.7607 - val_loss: 7788.7380 - val_psnr: -38.8965\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 11s 154ms/step - loss: 7601.5776 - psnr: -38.7718 - val_loss: 7060.3083 - val_psnr: -38.4650\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 11s 153ms/step - loss: 7574.6849 - psnr: -38.7782 - val_loss: 7123.9875 - val_psnr: -38.5029\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 11s 160ms/step - loss: 7600.2578 - psnr: -38.7782 - val_loss: 7791.6763 - val_psnr: -38.8959\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 11s 158ms/step - loss: 7576.5099 - psnr: -38.6803 - val_loss: 7191.1765 - val_psnr: -38.5464\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 11s 155ms/step - loss: 7610.1573 - psnr: -38.7163 - val_loss: 7509.9858 - val_psnr: -38.7362\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 11s 160ms/step - loss: 7564.8797 - psnr: -38.6674 - val_loss: 7784.2524 - val_psnr: -38.8906\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 11s 158ms/step - loss: 7578.4555 - psnr: -38.7448 - val_loss: 7150.2683 - val_psnr: -38.5235\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 11s 161ms/step - loss: 7558.8909 - psnr: -38.7749 - val_loss: 7180.8174 - val_psnr: -38.5379\n",
      "10/10 [==============================] - 1s 53ms/step\n",
      "[INFO] MSE:7576.95166015625   PSNRLoss:-38.79494857788086\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def psnr(y_true, y_pred):\n",
    "    '''assert y_true.shape == y_pred.shape, \"Cannot calculate PSNR. Input shapes not same.\" \\\n",
    "                                         \" y_true shape = %s, y_pred shape = %s\" % (str(y_true.shape),\n",
    "                                                                                   str(y_pred.shape))\n",
    "    '''\n",
    "    \"\"\"\n",
    "    PSNR is Peek Signal to Noise Ratio, which is similar to mean squared error.\n",
    "    It can be calculated as\n",
    "    PSNR = 20 * log10(MAXp) - 10 * log10(MSE)\n",
    "    When providing an unscaled input, MAXp = 255. Therefore 20 * log10(255)== 48.1308036087.\n",
    "    However, since we are scaling our input, MAXp = 1. Therefore 20 * log10(1) = 0.\n",
    "    Thus we remove that component completely and only compute the remaining MSE component.\n",
    "    \"\"\"\n",
    "    return -10. * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.)\n",
    "\n",
    "\n",
    "\n",
    "#compile the model algong with adam optimiser along with PSNR/SSIM loss metric\n",
    "model.compile(optimizer=adam(0.01),metrics=[psnr],loss='mse')\n",
    "model.fit(train_in,train_out,batch_size=10,nb_epoch=50,validation_data=(dev_in,dev_out))\n",
    "'''#loading saved weights\n",
    "modelWts=model.load_weights('savedWeightsCNN.h5')\n",
    "'''\n",
    "\n",
    "#evaluate the model\n",
    "score=model.evaluate(test_in,test_out)\n",
    "print(\"[INFO] MSE:{0}   PSNRLoss:{1}\".format(score[0],score[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peredicted resolution shape : [[[52.817448 48.997665 47.04327 ]\n",
      "  [59.8298   54.90462  54.360523]\n",
      "  [62.118942 56.859947 56.37574 ]\n",
      "  ...\n",
      "  [49.273613 53.5224   50.623726]\n",
      "  [39.359886 43.03825  40.51571 ]\n",
      "  [29.536192 32.55811  30.476841]]\n",
      "\n",
      " [[68.99388  62.29484  60.450085]\n",
      "  [76.531876 68.55346  68.74013 ]\n",
      "  [79.21521  71.00317  70.6515  ]\n",
      "  ...\n",
      "  [61.248306 66.87212  63.431763]\n",
      "  [49.7593   54.68743  51.58078 ]\n",
      "  [37.528683 41.592438 39.063408]]\n",
      "\n",
      " [[81.44875  73.381905 71.98701 ]\n",
      "  [88.56032  78.85114  80.243324]\n",
      "  [91.660194 81.4537   81.87506 ]\n",
      "  ...\n",
      "  [70.818596 78.50235  74.174   ]\n",
      "  [58.276447 65.056175 61.10417 ]\n",
      "  [44.240467 49.754925 46.50633 ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  ...\n",
      "  [77.98389  87.82188  88.73681 ]\n",
      "  [65.01819  73.77387  74.192024]\n",
      "  [49.072052 56.039585 56.23594 ]]\n",
      "\n",
      " [[ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  ...\n",
      "  [78.61438  90.388176 92.979164]\n",
      "  [64.711525 74.815544 76.88508 ]\n",
      "  [47.904892 55.836075 57.066   ]]\n",
      "\n",
      " [[ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  [ 0.        0.        0.      ]\n",
      "  ...\n",
      "  [81.70665  90.667564 94.326256]\n",
      "  [67.02403  74.48618  77.36297 ]\n",
      "  [49.48797  55.42083  57.240154]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#print(\"TEST image shape: \",test_in[0].shape) \n",
    "\n",
    "# prediction\n",
    "\n",
    "#unknown test data\n",
    "#cv2.imshow(\"Original Img\",test_original_resolution[0])\n",
    "#cv2.imshow(\"Low resolution\",test_imgs[0])\n",
    "pred_image=model.predict(test_in[4:5])\n",
    "'''cv2.imshow(\"Peredicted resolution\",pred_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()'''\n",
    "\n",
    "\n",
    "print(\"Peredicted resolution shape :\",pred_image[0])\n",
    "\n",
    "'''#save img\n",
    "plt.subplot(221)\n",
    "plt.imshow(test_in[2])\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(test_out[2])\n",
    "\n",
    "plt.subplot(223)\n",
    "'''\n",
    "plt.imshow(pred_image[0])\n",
    "\n",
    "\n",
    "\n",
    "cv2.imwrite(\"Original_Img4.jpg\",test_out[4])\n",
    "cv2.imwrite(\"Input_resolution4.jpg\",test_in[4])\n",
    "cv2.imwrite(\"Test_Output4.jpg\",pred_image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for i in range(0,len(test_in)):\n",
    "    cv2.imwrite(\"{0}.png\".format(i),test_in[i])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 100, 90, 64)       15616     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 90, 64)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 100, 90, 32)       2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 90, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 100, 90, 3)        2403      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100, 90, 3)        0         \n",
      "=================================================================\n",
      "Total params: 20,099\n",
      "Trainable params: 20,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Save the model\n",
    "model.save('resoluteitmodelCNN_F.h5')\n",
    "jsonmodel=model.to_json()\n",
    "model.save_weights('savedWeightsCNN_F.h5')\n",
    "\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#loading saved weights\n",
    "#modelWts=model.load_weights('savedWeightsCNN.h5')\n",
    "#model.get_weights()\n",
    "#model.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
